# Configuration for experiments
dims: 10
search_method: da
obj_func_name: ackley
num_acquisitions: 20
num_samples_per_acquisition: 20
surrogate: ackley_surrogate
num_init_samples: 200
rollout_round: 100
mode: fast
func_args: {}
surrogate_args: {}


# Hyper-parameters for different DFO methods
search_method_args:
  turbo:
    n_trust_regions: 5        # Number trust regions: TuRBO-1 will be used if set to 1, otherwise TuRBOM will be used
    n_repeat: 1               # Number repeat time for the same condition
    batch_size: 1             # How large batch size TuRBO uses
    verbose: True             # Print information from each batch
    use_ard: True             # Set to true if you want to use ARD for the GP kernel 
    max_cholesky_size: 2000   # When we switch from Cholesky to Lanczos
    n_training_steps: 50      # Number of steps of ADAM to learn the hypers
    min_cuda: 1024            # Run on the CPU for small datasets
    device: cpu               # "cpu" or "cuda"
    dtype: float32            # float64 or float32

  saasbo:
    device: cuda
    dtype: float32
    warmup_steps: 256
    thinning: 16
    num_mcmc_samples: 128

  lamcts:
    Cp: 1                     # Cp for MCTS
    leaf_size: 10             # Tree leaf size
    kernel_type: linear       # SVM configruation
    gamma_type: auto          # SVM configruation

  ipopcmaes:
    restarts: 5              # Number of restarts
    incpopsize: 2            # Multiplier for increasing population size before each restart.

  bipopcmaes:
    restarts: 5              # Number of restarts (the recommended setting is ``restarts <= 9``)
    incpopsize: 2            # Multiplier for increasing population size before each restart.
    bipop: True

  da:
    initial_temp: 0.05

  doo:
    explr_p: 0.01

  voo:
    explr_p: 0.001
    sampling_mode: centered_uniform
    switch_counter: 100